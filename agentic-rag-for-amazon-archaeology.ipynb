{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":101597,"databundleVersionId":12334818,"sourceType":"competition"},{"sourceId":12286664,"sourceType":"datasetVersion","datasetId":7743333},{"sourceId":12286690,"sourceType":"datasetVersion","datasetId":7743349},{"sourceId":12286854,"sourceType":"datasetVersion","datasetId":7743468},{"sourceId":12296863,"sourceType":"datasetVersion","datasetId":7750449},{"sourceId":12296867,"sourceType":"datasetVersion","datasetId":7750453},{"sourceId":12296886,"sourceType":"datasetVersion","datasetId":7750466}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anantaverma/agentic-rag-for-amazon-archaeology?scriptVersionId=247779468\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Agentic Multimodal RAG System for Archaeological Site Discovery  \n## OpenAI to Z Challenge\n\nThis notebook presents an Agentic Multimodal Retrieval-Augmented Generation (RAG) system designed to assist in the discovery and interpretation of potential archaeological sites within the Amazon basin. The system integrates satellite imagery, historical expedition texts, and agent-based reasoning to simulate the cognitive process of a human researcher generating, justifying, and evaluating archaeological hypotheses.\n\n## Dataset Overview\n\nWe utilize the following data sources:\n\n- **Image Tiles**: Pre-processed Amazonian satellite tiles for visual inspection.\n- **Textual Data**: Historical expedition logs and colonial documents.\n- **Shapefiles**: LiDAR-detected deforestation zones.\n- **Tile Bounds Metadata**: Geographic bounds for each tile.","metadata":{"_uuid":"400f295b-732b-47dd-a73c-d7bcc8eeef8b","_cell_guid":"d8e07304-6b4e-4e94-8034-16f0a320fd1e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Historical Text Data\n\nOur RAG pipeline draws on a curated set of digitized 19th-century exploration texts that document early European and American expeditions into the Amazon. These historical documents offer first-hand observations of indigenous cultures, environmental features, and settlement patterns that are invaluable for hypothesis generation.\n\n### Books Used in This Project:\n\n1. **Exploration of the Valley of the Amazon**\n2. **A Narrative of Travels on the Amazon and Rio Negro** \n3. **A Voyage up the River Amazon, Including a Residence at Pará**\n4. **The Amazon and Madeira Rivers**\n---\n\nThese texts were chunked and embedded using `SentenceTransformer`, enabling retrieval during the user query phase. Together, they serve as a **rich semantic layer of historical memory** from which our generative agent forms hypotheses.","metadata":{"_uuid":"d72d5219-d0ca-4688-af3e-47cc3400f665","_cell_guid":"4a5ed9b4-86c6-412f-b047-5771ba7c0353","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## System Architecture\n\nThe system follows a multimodal Agentic RAG pipeline:\n\n1. **User Query** — accepts natural language research questions.\n2. **Dual Retrieval** — fetches relevant text chunks and satellite image tiles using FAISS.\n3. **Geographic Expansion Agent** — suggests diverse river regions based on retrieval results.\n4. **LLM Hypothesis Agent** — proposes a detailed archaeological hypothesis.\n5. **Justification Agent** — validates hypothesis using text and image evidence.\n6. **Evaluation Agent** — assesses confidence and credibility.\n7. **Folium Map** — visualizes relevant tiles with shapefile overlays and previews.","metadata":{"_uuid":"ff5e069d-4ce8-4d92-85d5-249d0d985e46","_cell_guid":"07e64b86-203b-4b9c-8433-c8c7a1ce0716","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os\n\nfile_paths = []\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        file_paths.append(os.path.join(dirname, filename))\n\n# Print only summary + a few examples\nprint(f\"Total files found: {len(file_paths)}\")\nprint(\"Example files:\")\nfor path in file_paths[:5]:  \n    print(\"-\", path)","metadata":{"_uuid":"2cc676b3-176d-4a28-8e2e-47685f3e9688","_cell_guid":"df3b318c-28aa-4c76-a6f2-a14728fd66af","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T04:59:16.859558Z","iopub.execute_input":"2025-06-28T04:59:16.860042Z","iopub.status.idle":"2025-06-28T05:02:09.069637Z","shell.execute_reply.started":"2025-06-28T04:59:16.860006Z","shell.execute_reply":"2025-06-28T05:02:09.068245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## User Query and Multimodal Retrieval\n\nWe start with a natural language query. Our system retrieves:\n- Top `k` matching **text chunks** (via sentence-transformer)\n- Top `k` matching **image tiles** (via CLIP)","metadata":{"_uuid":"a21aa011-4f9b-421f-b2f0-eee88a3b1a67","_cell_guid":"d7d3ce2a-9d00-473f-982d-3af3a7bd4271","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Install and import all required packages\n!pip install -q geopandas rasterio matplotlib faiss-cpu torch folium textblob transformers sentence_transformers","metadata":{"_uuid":"4920eca2-de17-4ad5-a78c-c62dad8f2890","_cell_guid":"d8edaf4d-1c0a-4cb1-bfc2-e4de9107c190","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:02:09.071148Z","iopub.execute_input":"2025-06-28T05:02:09.07176Z","iopub.status.idle":"2025-06-28T05:03:57.676039Z","shell.execute_reply.started":"2025-06-28T05:02:09.071723Z","shell.execute_reply":"2025-06-28T05:03:57.674742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Install and import all required packages\nimport os\nimport pickle\nimport faiss\nimport torch\nimport folium\nimport geopandas as gpd\nimport numpy as np\nfrom textblob import TextBlob\nfrom transformers import CLIPProcessor, CLIPModel\nfrom sentence_transformers import SentenceTransformer\nfrom IPython.display import display, Markdown","metadata":{"_uuid":"e23a2785-45c3-4f9d-9e62-6d23e464ce4f","_cell_guid":"8969c024-5a8b-455f-ae68-4015c2a00dad","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:03:57.677656Z","iopub.execute_input":"2025-06-28T05:03:57.677986Z","iopub.status.idle":"2025-06-28T05:04:41.148007Z","shell.execute_reply.started":"2025-06-28T05:03:57.677946Z","shell.execute_reply":"2025-06-28T05:04:41.146908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading Embeddings and Models","metadata":{"_uuid":"f1e5c677-61da-4adc-8369-b9aebf96fe1c","_cell_guid":"ef960307-c6da-48c2-8e63-84f1333a1efc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load embeddings, indices, and metadata for both modalities\nCLIP_INDEX_PATH = \"/kaggle/input/embeddings/clip_tile_index.faiss\"\nCLIP_META_PATH = \"/kaggle/input/embeddings/clip_tile_metadata.pkl\"\nTEXT_INDEX_PATH = \"/kaggle/input/embeddings/text_index.faiss\"\nTEXT_META_PATH = \"/kaggle/input/embeddings/text_metadata.pkl\"\nTILE_BOUNDS_PATH = \"/kaggle/input/tile-bounds/tile_bounds.pkl\"\nTILE_DIR = \"/kaggle/input/tiles/tiles\"\nLIDAR_SHP_DIR = \"/kaggle/input/lidar-deforestation\"","metadata":{"_uuid":"3fbea223-3c94-4c9e-a538-519fca25ffda","_cell_guid":"74b14252-682b-44f2-b11c-5094c813d42e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:04:41.150745Z","iopub.execute_input":"2025-06-28T05:04:41.151676Z","iopub.status.idle":"2025-06-28T05:04:41.157174Z","shell.execute_reply.started":"2025-06-28T05:04:41.151646Z","shell.execute_reply":"2025-06-28T05:04:41.155979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------- LOAD MODELS ----------\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\ntext_model = SentenceTransformer(\"all-MiniLM-L6-v2\")","metadata":{"_uuid":"86dad3f1-ab05-4392-9870-ef8e93508258","_cell_guid":"243667f6-6b72-4655-b520-74684145fb62","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:04:41.158139Z","iopub.execute_input":"2025-06-28T05:04:41.15846Z","iopub.status.idle":"2025-06-28T05:05:07.005323Z","shell.execute_reply.started":"2025-06-28T05:04:41.158434Z","shell.execute_reply":"2025-06-28T05:05:07.003811Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Storing the data in Faiss","metadata":{"_uuid":"c1dbd12e-c15d-4110-8bfa-4d84eaf2cf96","_cell_guid":"b969403e-021b-4517-9ef7-014b10d4e2b4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load FAISS indices\ndef load_faiss_index(index_path):\n    return faiss.read_index(index_path)\n\n# Load metadata\ndef load_pickle(path):\n    with open(path, 'rb') as f:\n        return pickle.load(f)\n\n# Load everything\nclip_index = load_faiss_index(CLIP_INDEX_PATH)\nclip_meta = load_pickle(CLIP_META_PATH)\n\ntext_index = load_faiss_index(TEXT_INDEX_PATH)\ntext_meta = load_pickle(TEXT_META_PATH)\n\ntile_bounds = load_pickle(TILE_BOUNDS_PATH)\n\n# Confirm loading\nprint(f\"CLIP Index size: {clip_index.ntotal}\")\nprint(f\"Text Index size: {text_index.ntotal}\")\nprint(f\"Number of tile bounds: {len(tile_bounds)}\")","metadata":{"_uuid":"e4d3795b-ed65-4944-a641-0379f7aef1eb","_cell_guid":"cf89a085-4d59-4a56-8c32-0e57d520d760","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:07.006531Z","iopub.execute_input":"2025-06-28T05:05:07.006821Z","iopub.status.idle":"2025-06-28T05:05:09.368578Z","shell.execute_reply.started":"2025-06-28T05:05:07.006797Z","shell.execute_reply":"2025-06-28T05:05:09.366915Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Textblob for query check","metadata":{"_uuid":"946f4594-c22b-436e-9573-afb584de3734","_cell_guid":"711d1c7b-ef12-409e-9b96-0f70795f3a66","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ---------- SPELL CORRECT ----------\ndef correct_query(query):\n    return str(TextBlob(query).correct())\n\n# ---------- EMBED QUERY ----------\ndef embed_query_clip(query):\n    inputs = clip_processor(text=query, return_tensors=\"pt\", padding=True).to(device)\n    with torch.no_grad():\n        vector = clip_model.get_text_features(**inputs).cpu().numpy().astype(\"float32\")\n    return vector\n\ndef embed_query_text(query):\n    return text_model.encode([query]).astype(\"float32\")","metadata":{"_uuid":"0ba09aaa-1375-4266-9639-5102be4e1d64","_cell_guid":"2a47825a-bc1b-4999-b23c-134ef29d9ee7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:09.36989Z","iopub.execute_input":"2025-06-28T05:05:09.370374Z","iopub.status.idle":"2025-06-28T05:05:09.378955Z","shell.execute_reply.started":"2025-06-28T05:05:09.370338Z","shell.execute_reply":"2025-06-28T05:05:09.377535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Embedding the user query","metadata":{"_uuid":"9e79e7e8-412a-41b2-b95a-1b383806b6a6","_cell_guid":"46162d93-501c-45e4-b38d-a8101746f541","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load SentenceTransformer model\ntext_embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Function to query text index\ndef retrieve_text_chunks(query, k=10):\n    query_embed = text_embedder.encode([query])\n    D, I = text_index.search(query_embed, k)\n\n    text_chunks = text_meta[\"texts\"]\n    results = [text_chunks[i] if i < len(text_chunks) else f\"[Missing chunk {i}]\" for i in I[0]]\n    return results","metadata":{"_uuid":"da393388-52cc-4d56-a889-a0d87451dfb9","_cell_guid":"38e3f03d-3034-4a2d-aa34-80d726315b66","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:09.380165Z","iopub.execute_input":"2025-06-28T05:05:09.38064Z","iopub.status.idle":"2025-06-28T05:05:10.560556Z","shell.execute_reply.started":"2025-06-28T05:05:09.380611Z","shell.execute_reply":"2025-06-28T05:05:10.559593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load CLIP Text Encoder\nfrom transformers import CLIPProcessor, CLIPModel\n\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Function to encode query with CLIP\ndef encode_clip_text(text):\n    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        return clip_model.get_text_features(**inputs).cpu().numpy()\n\n# Function to query image tile index\ndef retrieve_tile_ids(query, k=10):\n    query_embed = encode_clip_text(query)\n    D, I = clip_index.search(query_embed, k)\n    results = [clip_meta[i] for i in I[0]]\n    return results","metadata":{"_uuid":"a6daf1fe-a9e0-48a7-b0da-cd416bd82f87","_cell_guid":"97e52711-f079-4d3b-9f8d-e7768c63685e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:10.561996Z","iopub.execute_input":"2025-06-28T05:05:10.56232Z","iopub.status.idle":"2025-06-28T05:05:13.461806Z","shell.execute_reply.started":"2025-06-28T05:05:10.562292Z","shell.execute_reply":"2025-06-28T05:05:13.460543Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{"_uuid":"9144f2b3-ed27-4557-a76e-60f5cec92897","_cell_guid":"752f4a1d-a750-442f-864a-21fe370956f3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Generative Component of the RAG Pipeline","metadata":{"_uuid":"a56800ea-cf23-49cf-a6dc-2a5115a20bb9","_cell_guid":"7eececcf-6afb-40df-986c-e0b521c07112","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from openai import OpenAI\nimport os\nimport openai\n\n# If using Kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"OPENAI_API_KEY\"] = user_secrets.get_secret(\"OPENAI_API_KEY\")\n\n# Initialize the client using your Kaggle environment variable\nclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))","metadata":{"_uuid":"d655b7d8-fff1-49c1-b64a-fc87223840bc","_cell_guid":"87a53f57-c227-4216-857c-e7b28ed64548","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:13.465476Z","iopub.execute_input":"2025-06-28T05:05:13.465868Z","iopub.status.idle":"2025-06-28T05:05:17.310849Z","shell.execute_reply.started":"2025-06-28T05:05:13.465845Z","shell.execute_reply":"2025-06-28T05:05:17.309878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef run_geographic_expansion_agent(query, retrieved_texts):\n    expansion_prompt = f\"\"\"\nYou are an archaeological reasoning assistant. The user asked:\n\n\"{query}\"\n\nYou have access to historical text excerpts and satellite imagery tile references that may contain clues about river proximity, terrain, and prior settlements.\n\nYour task is to:\n- Analyze the following text chunks and satellite tile IDs\n- Identify signs of multiple **distinct river systems or regions** mentioned or implied\n- Propose 2–3 **alternative Amazon river regions** that could also be promising areas for human settlement based on the data retrieved or archaeological significance.\n\nOnly suggest rivers or regions if they are either:\n- mentioned in the text, OR\n- clearly inferred from the distribution or features in the imagery\n\nHistorical Texts:\n{chr(10).join(['- ' + t for t in retrieved_texts])}\n\"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": expansion_prompt}],\n        temperature=0.5\n    )\n    return response.choices[0].message.content.strip()","metadata":{"_uuid":"3c4b3826-59dd-4c4d-82a8-626f5ee4f2a4","_cell_guid":"f739adda-7e06-4f88-ae2d-e8f027a1f856","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:17.311941Z","iopub.execute_input":"2025-06-28T05:05:17.31234Z","iopub.status.idle":"2025-06-28T05:05:18.458798Z","shell.execute_reply.started":"2025-06-28T05:05:17.312309Z","shell.execute_reply":"2025-06-28T05:05:18.45758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The core LLM agent uses the query, text, image tile matches, and suggested regions\nto generate a scientific hypothesis grounded in historical and visual data.","metadata":{"_uuid":"22763f18-c6fa-47bc-bb53-ca62a487abfd","_cell_guid":"0a23d6e7-dbb7-41b0-9665-bfc7fd5c6dd6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import openai\nimport os\n\n# 🔐 Load OpenAI API key from Kaggle secrets or environment\nopenai.api_key = os.environ.get(\"OPENAI_API_KEY\")  # Ensure this is set in Kaggle secrets\n\n# ✅ Hypothesis generator with structured response\ndef generate_hypothesis(query, text_chunks, top_tiles=None, expanded_regions=None):\n    tile_references = \"\"\n    if top_tiles:\n        tile_references = \"\\n\".join([f\"- {tile}\" for tile in top_tiles[:5]])\n\n    prompt = f\"\"\"\nYou are an expert archaeological assistant with access to both historical texts and satellite imagery.\n\nUser query: {query}\n\nYou are given the following historical text excerpts:\n{text_chunks}\n\nYou are also provided with satellite image tiles that exhibit notable landscape features:\n{tile_references}\n\"\"\"\n    if expanded_regions:\n        prompt += f\"\"\"\n\nAdditionally, you are encouraged to consider these other potentially relevant regions based on known Amazonian settlement patterns:\n{expanded_regions}\n\nBased on both the historical text and image references, provide a detailed and academically grounded hypothesis about what these features may represent. Be specific in your analysis, referencing past known civilizations, agricultural or ceremonial patterns, or any anthropogenic shapes like rectangles, grids, or radial structures that are visible.\n\nStructure your output like this:\n**Hypothesis**\n...\n\n**Justification**\n...\n\nUse clear language that could be understood by a research reviewer or geospatial archaeologist.\n\"\"\"\n\n    client = openai.OpenAI()\n\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.4\n    )\n    \n    return response.choices[0].message.content.strip()","metadata":{"_uuid":"fb8430eb-f2ce-42fe-a82f-1625a0737b33","_cell_guid":"f7db4b61-8167-4fb9-a0a9-3f37ae3800f5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:18.459968Z","iopub.execute_input":"2025-06-28T05:05:18.460359Z","iopub.status.idle":"2025-06-28T05:05:19.756091Z","shell.execute_reply.started":"2025-06-28T05:05:18.46033Z","shell.execute_reply":"2025-06-28T05:05:19.755121Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Map Integration","metadata":{"_uuid":"4e3b5e9c-7cc6-420f-bf07-fc44eeb236c3","_cell_guid":"c8965b03-93d6-4d7e-be08-5f345b7eb296","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import folium\nfrom folium import IFrame\nimport base64\nimport os\nimport geopandas as gpd\nimport branca.colormap as cm\n\ndef render_tile_map(tile_ids, tile_bounds, hypothesis=None, confidence_scores=None):\n    default_location = [-3.4653, -62.2159]\n    fmap = folium.Map(location=default_location, zoom_start=5)\n\n    for i, tile_id in enumerate(tile_ids):\n        if tile_id not in tile_bounds:\n            continue\n        lat_min, lon_min, lat_max, lon_max = tile_bounds[tile_id]\n        bounds = [[lat_min, lon_min], [lat_max, lon_max]]\n\n        # Load image and encode as base64\n        image_path = os.path.join(TILE_DIR, tile_id)\n        encoded_img = \"\"\n        if os.path.exists(image_path):\n            with open(image_path, 'rb') as f:\n                encoded_img = base64.b64encode(f.read()).decode()\n        \n        # Optional confidence for color\n        score = confidence_scores[i] if confidence_scores else 0.5\n        color = \"green\" if score >= 0.75 else \"orange\" if score >= 0.5 else \"red\"\n\n        html = f\"<b>{tile_id}</b><br>\"\n        if encoded_img:\n            html += f'<img src=\"data:image/png;base64,{encoded_img}\" width=\"200\" />'\n\n        popup = folium.Popup(IFrame(html, width=210, height=220), max_width=250)\n\n        folium.Rectangle(\n            bounds=bounds,\n            color=color,\n            fill=True,\n            fill_opacity=0.4,\n            popup=popup\n        ).add_to(fmap)\n\n    if hypothesis:\n        folium.Marker(\n            location=default_location,\n            icon=folium.Icon(color='green'),\n            popup=hypothesis[:300]\n        ).add_to(fmap)\n\n    return fmap\n    \ndef add_shapefile_overlay(fmap, shp_dir, max_features=500):\n    for fname in os.listdir(shp_dir):\n        if fname.endswith(\".shp\"):\n            path = os.path.join(shp_dir, fname)\n            gdf = gpd.read_file(path)\n            if len(gdf) > max_features:\n                gdf = gdf.sample(max_features)  # subsample\n            folium.GeoJson(gdf, name=fname).add_to(fmap)\n    return fmap","metadata":{"_uuid":"a517f615-f252-43a4-a7db-2c71655d8744","_cell_guid":"d86b90d1-2038-4207-a50b-96845e21ae4a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:19.757361Z","iopub.execute_input":"2025-06-28T05:05:19.757742Z","iopub.status.idle":"2025-06-28T05:05:20.259537Z","shell.execute_reply.started":"2025-06-28T05:05:19.757712Z","shell.execute_reply":"2025-06-28T05:05:20.258464Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Agents for Justification and Evaluation\n\nWe use two additional agents:\n- **Justification Agent**: Checks alignment of hypothesis with source data\n- **Evaluation Agent**: Adds a confidence score and flags uncertainty","metadata":{"_uuid":"9c76085c-ca8b-4f51-a097-64219771dbab","_cell_guid":"15e3667d-9d0a-4fad-bae0-5de0529158d4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def run_justification_agent(query, text_chunks, tile_ids, llm_hypothesis):\n    justification_prompt = f\"\"\"\nYou are a scientific reasoning agent. Your task is to verify whether the following hypothesis is well-supported by the provided data.\n\nUser Query:\n{query}\n\nHypothesis:\n{llm_hypothesis}\n\nSupporting Text Chunks:\n{chr(10).join(['- ' + chunk for chunk in text_chunks])}\n\nReferenced Image Tile IDs:\n{tile_ids}\n\nPlease return a structured justification. Highlight which parts of the hypothesis are strongly supported, weakly supported, or unsupported based on the data.\n\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": justification_prompt}],\n        temperature=0.3\n    )\n\n    return response.choices[0].message.content.strip()","metadata":{"_uuid":"aa2be007-be51-4cbf-9490-2c2f921ae812","_cell_guid":"87a9c1e3-29dc-4bf8-8b9f-8b112d786100","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:20.260725Z","iopub.execute_input":"2025-06-28T05:05:20.261017Z","iopub.status.idle":"2025-06-28T05:05:20.284318Z","shell.execute_reply.started":"2025-06-28T05:05:20.260995Z","shell.execute_reply":"2025-06-28T05:05:20.283271Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def run_evaluation_agent(hypothesis, justification):\n    evaluation_prompt = f\"\"\"\nYou are an evaluator AI assessing the scientific credibility of a hypothesis and its justification.\n\nHypothesis:\n{hypothesis}\n\nJustification:\n{justification}\n\nPlease evaluate the overall credibility and clarity of the hypothesis. Return:\n\n### Evaluation\n- Confidence Score (1–10): X/10\n- Major Strengths:\n- Limitations or Uncertainty Factors:\n- Should this be pursued further? (Yes/No and Why)\n\"\"\"\n    response = client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=[{\"role\": \"user\", \"content\": evaluation_prompt}],\n        temperature=0.3\n    )\n\n    return response.choices[0].message.content.strip()","metadata":{"_uuid":"e7aa4b6e-a85e-4d01-baf1-7e104c81fc1e","_cell_guid":"2e9f8933-b2c6-4723-ad0e-9811ac2be68e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:20.285601Z","iopub.execute_input":"2025-06-28T05:05:20.28591Z","iopub.status.idle":"2025-06-28T05:05:20.309918Z","shell.execute_reply.started":"2025-06-28T05:05:20.285888Z","shell.execute_reply":"2025-06-28T05:05:20.308889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## User Query and Multimodal RAG\n\nWe start with a natural language query. Our system retrieves:\n- Top `k` matching **text chunks** (via sentence-transformer)\n- Top `k` matching **image tiles** (via CLIP)\n\nFinal Hypothesis, Justification, and Evaluation","metadata":{"_uuid":"9e5db4f2-2fa0-4a7a-8a90-dd1e9f3e9733","_cell_guid":"893128f7-a1bb-4c26-88e1-62ec2e010063","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"#change your query here\nquery = \"Where might ancient settlements exist near major rivers in the Amazon?\"\n\n# Step 1: Retrieve\ntext_results = retrieve_text_chunks(query, k=10)\nimage_results = retrieve_tile_ids(\"circular mound structures\", k=10)\n\n# Step 2: Expand regionally\nexpanded_regions = run_geographic_expansion_agent(query, text_results)\n\n# Step 3: Generate hypothesis\nhypothesis_output = generate_hypothesis(\n    query=query,\n    text_chunks=\"\\n\\n\".join(text_results),\n    top_tiles=image_results,\n    expanded_regions=expanded_regions\n)\n\n# Step 4: Justify + Evaluate\njustification = run_justification_agent(query, text_results, image_results, hypothesis_output)\nevaluation = run_evaluation_agent(hypothesis_output, justification)\n\n# map_output = add_shapefile_overlay(map_output, LIDAR_SHP_DIR)\n# display(map_output)\n\n# Step 6: Show output\nprint(\"Hypothesis:\\n\", hypothesis_output)\nprint(\"\\nJustification:\\n\", justification)\nprint(\"\\nEvaluation:\\n\", evaluation)","metadata":{"_uuid":"5fe3a7de-7f1c-4b42-a7ff-d36db9504841","_cell_guid":"3b274b1f-0e3f-4562-9bd1-a028427806eb","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:05:20.311345Z","iopub.execute_input":"2025-06-28T05:05:20.311706Z","iopub.status.idle":"2025-06-28T05:06:17.498471Z","shell.execute_reply.started":"2025-06-28T05:05:20.31168Z","shell.execute_reply":"2025-06-28T05:06:17.496645Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Interactive Map and Visualization\n\n- Tiles are shown with bounding rectangles\n- Deforestation shapefiles are overlaid for contextual insights","metadata":{"_uuid":"1b4a5530-1db5-46cf-984a-43ea844ca574","_cell_guid":"72d8c0e3-5cee-4a07-9461-57dd06ba1f27","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Display top-k image tile matches as thumbnails for visual reference\nfrom IPython.display import display\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\n\ndef show_top_tiles(tile_ids, tile_dir=TILE_DIR, size=(150, 150)):\n    \"\"\"\n    Display top image tiles used in RAG with resized thumbnails.\n    \"\"\"\n    num_tiles = min(len(tile_ids), 10)\n    fig, axes = plt.subplots(1, num_tiles, figsize=(num_tiles * 2, 2.5))\n    \n    if num_tiles == 1:\n        axes = [axes]  # ensure iterable if only one tile\n    \n    for i in range(num_tiles):\n        tile_path = os.path.join(tile_dir, tile_ids[i])\n        try:\n            img = Image.open(tile_path).resize(size)\n            axes[i].imshow(img)\n            axes[i].set_title(tile_ids[i][:20], fontsize=8)\n            axes[i].axis(\"off\")\n        except Exception as e:\n            axes[i].set_title(\"Missing\", fontsize=8)\n            axes[i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_uuid":"d60f532d-b9db-4ea7-8b09-c9f1414256b6","_cell_guid":"0ba649bf-4464-4bbe-bbd7-90871e94e9c6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:06:17.500297Z","iopub.execute_input":"2025-06-28T05:06:17.500695Z","iopub.status.idle":"2025-06-28T05:06:17.509926Z","shell.execute_reply.started":"2025-06-28T05:06:17.500667Z","shell.execute_reply":"2025-06-28T05:06:17.508631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_top_tiles(image_results)","metadata":{"_uuid":"ba20cbce-cf9b-4a2a-823a-45ede566217d","_cell_guid":"0b43d4e8-0976-4e97-8ddd-b3f92ecb47af","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:06:17.511252Z","iopub.execute_input":"2025-06-28T05:06:17.511997Z","iopub.status.idle":"2025-06-28T05:06:18.45307Z","shell.execute_reply.started":"2025-06-28T05:06:17.51196Z","shell.execute_reply":"2025-06-28T05:06:18.451743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display\n\ntile_ids = image_results\n\nmap_out = render_tile_map(tile_ids, tile_bounds, hypothesis=hypothesis_output)\n\n# Add .shp overlays (optional)\nmap_out = add_shapefile_overlay(map_out, LIDAR_SHP_DIR)\n\n# Display\ndisplay(map_out)","metadata":{"_uuid":"dfcc34d8-25dc-4b7c-a542-86fae7253cf1","_cell_guid":"c009c6db-cd67-48ff-9a6c-1f123dec1a1d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-06-28T05:06:18.454625Z","iopub.execute_input":"2025-06-28T05:06:18.454981Z","iopub.status.idle":"2025-06-28T05:10:24.803368Z","shell.execute_reply.started":"2025-06-28T05:06:18.454954Z","shell.execute_reply":"2025-06-28T05:10:24.801818Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Map View: Satellite Tile Predictions + Deforestation Activity\n\nThis map visualizes the system’s reasoning outputs in geospatial form:\n\n### 🟠 Orange Rectangles\n\n* Represent **top-matching satellite image tiles** retrieved by CLIP for the given user query.\n* These are the regions the model considers visually relevant (e.g., showing circular mounds or rectilinear structures) for archaeological investigation.\n\n### 🟢 Green Marker\n\n* Indicates the **center of hypothesis generation**.\n* It marks the approximate center of the region the LLM focused on when generating its archaeological hypothesis.\n\n### 🔵 Blue Dots\n\n* These come from **deforestation shapefile overlays** (e.g., `deter-amz-deter-public.shp`).\n* Each polygon or point represents a **recorded deforestation event** or **burn scar**, classified as:\n\n  * `CICATRIZ_DE_QUEIMADA` – Burned scar\n  * `CS_GEOMETRICO` – Geometric clearing\n* Many of these occur even within protected areas, and may coincide with or obscure potential archaeological features.","metadata":{"_uuid":"8e9b6fc6-d19e-4601-bca7-b91cac044dbd","_cell_guid":"1c94bf1f-c66a-47f5-bc9d-5acd9ad4a851","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Conclusion, Limitations and Future Work\n\nThis notebook demonstrates a novel use of multimodal RAG with agents to form scientifically plausible hypotheses about past civilizations. While this system shows strong potential, there are several important limitations:\n\n* **Data Scale**: The dataset is small, ie, only a few historical texts and limited satellite tiles as compared to the vast archaeological possibilities across the Amazon.\n* **Static Inputs**: All embeddings are precomputed; no real-time data or user-uploaded imagery is currently supported.\n* **Limited Accuracy**: Hypotheses are not validated against known archaeological site databases or fieldwork.\n* **Modal Simplicity**: We use only RGB imagery\n* **LLM Fragility**: Despite agentic structuring, reasoning can still be speculative without deeper fine-tuning or constraints.\n\nThis prototype demonstrates the feasibility of an agentic RAG workflow for archaeology, and offers a strong foundation for future, more robust systems.","metadata":{"_uuid":"a5fbff4c-2c73-4d39-b026-94cda3d01bda","_cell_guid":"3ef55145-daf1-4ea4-b2c3-43242759ab9a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}